{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SyntaxDiffusion/SyntaxNodes/blob/main/braindead_stable_audio_open_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iW4GSyVnc8jW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "04e31ce1-da57-4f86-dc14-1074c475ad13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "# @title download and install\n",
        "!git clone https://github.com/Stability-AI/stable-audio-tools.git\n",
        "%cd /content/stable-audio-tools\n",
        "!pip install -e .\n",
        "%cd ..\n",
        "!apt-get update -y\n",
        "!apt-get install ffmpeg -y\n",
        "\n",
        "!python -m pip install -U pip setuptools wheel\n",
        "!python -m pip install --force-reinstall https://github.com/yt-dlp/yt-dlp/archive/master.tar.gz\n",
        "\n",
        "!gdown 1a--6MqPu8PiDfxXoXPHok79kxCxrqrmW # model.safetensors\n",
        "\n",
        "!pip install -U protobuf soundfile gdown wandb\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "\n",
        "!wandb login\n",
        "#@markdown You'll need a wandb key to log training info, get one at https://wandb.ai/authorize and paste it in when asked\n",
        "\n",
        "#@markdown If colab asks if you want to restart, *do not restart* instead press `cancel`.\n",
        "\n",
        "#@markdown If colab gave you a T4 or L4 instead of A100, it will OOM immediately when training starts. Training stable audio open requires **at least** 27.6 gb vram, which an A100 has (40gb), but both of the other gpu types are too small (16 and 24gb). Restart until you get an A100, or else this notebook won't work.\n",
        "\n",
        "#@markdown Alternatively, you can run this on other gpu clouds (eg, [runpod](https://www.runpod.io/console/deploy)) but you'll need to make modifications to the code for it to work there (replacing every instance of `/content/` with `/workspace/`, etc)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title scrape\n",
        "\n",
        "import os\n",
        "import yt_dlp\n",
        "\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "tqdm = partial(tqdm, position=0, leave=True)\n",
        "\n",
        "youtube_playlist_link = \"https://www.youtube.com/playlist?list=PLZ4DbyIWUwCq4V8bIEa8jm2ozHZVuREJP\" # @param {type:\"string\"}\n",
        "\n",
        "dataset_path = \"/content/dataset\" # @param {type:\"string\"}\n",
        "os.makedirs(dataset_path, exist_ok=True)\n",
        "\n",
        "# youtube scraper\n",
        "ydl_opts = {\n",
        "    'format': 'bestaudio/best',\n",
        "    'outtmpl': os.path.join(dataset_path, '%(title)s.%(ext)s'),\n",
        "    'postprocessors': [{\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'mp3',\n",
        "        'preferredquality': '128',\n",
        "    }],\n",
        "    'quiet': True,\n",
        "    'extract_flat': True,\n",
        "    # 'force_generic_extractor': True,\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    info_dict = ydl.extract_info(youtube_playlist_link, download=False)\n",
        "    if 'entries' in tqdm(info_dict):\n",
        "        for i, entry in enumerate(info_dict['entries']):\n",
        "            print(f\"extracting {entry['title']} {entry['url']} ({i}/{len(info_dict['entries'])})\")\n",
        "            try:\n",
        "                ydl.download([entry['url']])\n",
        "            except:\n",
        "                print(f\"failed to download {entry['url']}\")\n",
        "\n",
        "print('done!')\n",
        "print(f'got {len(os.listdir(dataset_path))} songs')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "gQp03LbJ9zBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title train\n",
        "\n",
        "import math\n",
        "import json\n",
        "import os\n",
        "\n",
        "# create config json files\n",
        "\n",
        "seconds = 10 # @param {type:\"integer\"}\n",
        "\n",
        "def calculate_samples(seconds):\n",
        "    sample_rate = 44100\n",
        "    downsampling_ratio = 1024\n",
        "    total_samples = seconds * sample_rate\n",
        "    adjusted_samples = math.ceil(total_samples / downsampling_ratio) * downsampling_ratio\n",
        "    return adjusted_samples\n",
        "\n",
        "assert calculate_samples(10) == 441344\n",
        "\n",
        "sample_size = calculate_samples(seconds)\n",
        "\n",
        "random_crop = True # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown `random_crop` should be left enabled when training on full songs. If you're training on loops or oneshots, turn it off- but then you'll need to skip the scraping step and instead upload your samples to `/content/dataset/*`.\n",
        "\n",
        "demo_prompt_1 = \"A beautiful orchestral symphony, classical music\" # @param {type:\"string\"}\n",
        "demo_prompt_2 = \"A pop song about love and loss\" # @param {type:\"string\"}\n",
        "demo_prompt_3 = \"Chill hip-hop beat, chillhop\" # @param {type:\"string\"}\n",
        "demo_prompt_4 = \"Amen break 174 BPM\" # @param {type:\"string\"}\n",
        "\n",
        "#@markdown Note: this notebook uses the filename (in this case, video title) as the prompt. for songs, this will typically be in the format `Artist - Song Title`, or occasionally just `Song Title`. Take a look at the style the video titles in your playlist are written in, and mimic that for the above demo prompts. This is also how you will be prompting the finished model. If you would like to use a different prompting style, or use a captioner's output, you'll need to go edit the `caption_py` string near the bottom of this cell to include any functionality you need. This notebook is just a starting point!\n",
        "\n",
        "model_config = {\n",
        "    \"model_type\": \"diffusion_cond\",\n",
        "    \"sample_size\": sample_size,\n",
        "    \"sample_rate\": 44100,\n",
        "    \"audio_channels\": 2,\n",
        "    \"model\": {\n",
        "        \"pretransform\": {\n",
        "            \"type\": \"autoencoder\",\n",
        "            \"iterate_batch\": True,\n",
        "            \"config\": {\n",
        "                \"encoder\": {\n",
        "                    \"type\": \"oobleck\",\n",
        "                    \"requires_grad\": False,\n",
        "                    \"config\": {\n",
        "                        \"in_channels\": 2,\n",
        "                        \"channels\": 128,\n",
        "                        \"c_mults\": [1, 2, 4, 8, 16],\n",
        "                        \"strides\": [2, 4, 4, 8, 8],\n",
        "                        \"latent_dim\": 128,\n",
        "                        \"use_snake\": True\n",
        "                    }\n",
        "                },\n",
        "                \"decoder\": {\n",
        "                    \"type\": \"oobleck\",\n",
        "                    \"config\": {\n",
        "                        \"out_channels\": 2,\n",
        "                        \"channels\": 128,\n",
        "                        \"c_mults\": [1, 2, 4, 8, 16],\n",
        "                        \"strides\": [2, 4, 4, 8, 8],\n",
        "                        \"latent_dim\": 64,\n",
        "                        \"use_snake\": True,\n",
        "                        \"final_tanh\": False\n",
        "                    }\n",
        "                },\n",
        "                \"bottleneck\": {\n",
        "                    \"type\": \"vae\"\n",
        "                },\n",
        "                \"latent_dim\": 64,\n",
        "                \"downsampling_ratio\": 2048,\n",
        "                \"io_channels\": 2\n",
        "            }\n",
        "        },\n",
        "        \"conditioning\": {\n",
        "            \"configs\": [\n",
        "                {\n",
        "                    \"id\": \"prompt\",\n",
        "                    \"type\": \"t5\",\n",
        "                    \"config\": {\n",
        "                        \"t5_model_name\": \"t5-base\",\n",
        "                        \"max_length\": 128\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"id\": \"seconds_start\",\n",
        "                    \"type\": \"number\",\n",
        "                    \"config\": {\n",
        "                        \"min_val\": 0,\n",
        "                        \"max_val\": 512\n",
        "                    }\n",
        "                },\n",
        "                {\n",
        "                    \"id\": \"seconds_total\",\n",
        "                    \"type\": \"number\",\n",
        "                    \"config\": {\n",
        "                        \"min_val\": 0,\n",
        "                        \"max_val\": 512\n",
        "                    }\n",
        "                }\n",
        "            ],\n",
        "            \"cond_dim\": 768\n",
        "        },\n",
        "        \"diffusion\": {\n",
        "            \"cross_attention_cond_ids\": [\"prompt\", \"seconds_start\", \"seconds_total\"],\n",
        "            \"global_cond_ids\": [\"seconds_start\", \"seconds_total\"],\n",
        "            \"type\": \"dit\",\n",
        "            \"config\": {\n",
        "                \"io_channels\": 64,\n",
        "                \"embed_dim\": 1536,\n",
        "                \"depth\": 24,\n",
        "                \"num_heads\": 24,\n",
        "                \"cond_token_dim\": 768,\n",
        "                \"global_cond_dim\": 1536,\n",
        "                \"project_cond_tokens\": False,\n",
        "                \"transformer_type\": \"continuous_transformer\"\n",
        "            }\n",
        "        },\n",
        "        \"io_channels\": 64\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"use_ema\": True,\n",
        "        \"log_loss_info\": False,\n",
        "        \"optimizer_configs\": {\n",
        "            \"diffusion\": {\n",
        "                \"optimizer\": {\n",
        "                    \"type\": \"AdamW\",\n",
        "                    \"config\": {\n",
        "                        \"lr\": 5e-5,\n",
        "                        \"betas\": [0.9, 0.999],\n",
        "                        \"weight_decay\": 1e-3\n",
        "                    }\n",
        "                },\n",
        "                \"scheduler\": {\n",
        "                    \"type\": \"InverseLR\",\n",
        "                    \"config\": {\n",
        "                        \"inv_gamma\": 1000000,\n",
        "                        \"power\": 0.5,\n",
        "                        \"warmup\": 0.99\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        \"demo\": {\n",
        "            \"demo_every\": 2000,\n",
        "            \"demo_steps\": 250,\n",
        "            \"num_demos\": 4,\n",
        "            \"demo_cond\": [\n",
        "                {\"prompt\": f\"{demo_prompt_1}\", \"seconds_start\": 0, \"seconds_total\": seconds},\n",
        "                {\"prompt\": f\"{demo_prompt_2}\", \"seconds_start\": 0, \"seconds_total\": seconds},\n",
        "                {\"prompt\": f\"{demo_prompt_3}\", \"seconds_start\": 0, \"seconds_total\": seconds},\n",
        "                {\"prompt\": f\"{demo_prompt_4}\", \"seconds_start\": 0, \"seconds_total\": seconds}\n",
        "            ],\n",
        "            \"demo_cfg_scales\": [4, 8]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "dataset_config = {\n",
        "    \"dataset_type\": \"audio_dir\",\n",
        "    \"datasets\": [\n",
        "        {\n",
        "            \"id\": \"dataset\",\n",
        "            \"path\": dataset_path,\n",
        "            \"custom_metadata_module\": \"/content/caption.py\"\n",
        "        }\n",
        "    ],\n",
        "    \"random_crop\": random_crop\n",
        "}\n",
        "\n",
        "caption_py = \"\"\"import os\n",
        "\n",
        "def get_custom_metadata(info, audio):\n",
        "    caption = info[\"relpath\"]\n",
        "    caption = os.path.splitext(os.path.basename(caption))[0]\n",
        "    return {\"prompt\": caption}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"model_config.json\", \"w\") as f:\n",
        "    json.dump(model_config, f)\n",
        "\n",
        "with open(\"dataset_config.json\", \"w\") as f:\n",
        "    json.dump(dataset_config, f)\n",
        "\n",
        "with open(\"caption.py\", \"w\") as f:\n",
        "    f.write(caption_py)\n",
        "\n",
        "# actually train\n",
        "\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
        "\n",
        "command = (\n",
        "    \"python stable-audio-tools/train.py\"\n",
        "    \" --config-file stable-audio-tools/defaults.ini\"\n",
        "    \" --dataset-config /content/dataset_config.json\"\n",
        "    \" --model-config /content/model_config.json\"\n",
        "    \" --precision 16-mixed\"\n",
        "    \" --batch-size 1\"\n",
        "    \" --num-gpus 1\"\n",
        "    \" --num-workers 8\"\n",
        "    \" --seed 1234\"\n",
        "    \" --name train_stable_audio_open\"\n",
        "    \" --save-dir checkpoints\"\n",
        "    \" --checkpoint-every 2000\"\n",
        "    \" --pretrained-ckpt-path /content/model.safetensors\"\n",
        ")\n",
        "\n",
        "!{command}\n",
        "\n",
        "print('done!')"
      ],
      "metadata": {
        "id": "h0LhPnRpdPEi",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title unwrap and export\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "def find_last_modified_file(root_folder):\n",
        "    max_mtime = 0\n",
        "    max_file = None\n",
        "    for root, dirs, files in os.walk(root_folder):\n",
        "        for file in files:\n",
        "            full_path = os.path.join(root, file)\n",
        "            mtime = os.path.getmtime(full_path)\n",
        "            if mtime > max_mtime:\n",
        "                max_mtime = mtime\n",
        "                max_file = full_path\n",
        "    return max_file\n",
        "\n",
        "def extract_step_count(file_path):\n",
        "    match = re.search(r\"step=(\\d+)\", file_path)\n",
        "    if match:\n",
        "        return round(int(match.group(1)), -3)\n",
        "    return None\n",
        "\n",
        "checkpoint_path = find_last_modified_file('/content/checkpoints')\n",
        "\n",
        "if checkpoint_path is None:\n",
        "    print(\"no checkpoints found. \\nAre you sure the training cell ran long enough to save one? aka 2000 steps\")\n",
        "    exit()\n",
        "\n",
        "name = f\"stable_audio_open_finetuned_{extract_step_count(checkpoint_path)}k\"\n",
        "\n",
        "command = (\n",
        "    \"python stable-audio-tools/unwrap_model.py\"\n",
        "    \" --model-config /content/model_config.json\"\n",
        "    f\" --ckpt-path {checkpoint_path}\"\n",
        "    f\" --name {name}\"\n",
        ")\n",
        "\n",
        "!{command}\n",
        "\n",
        "unwrapped_checkpoint = f\"/content/{name}.ckpt\"\n",
        "\n",
        "print(\"done!\")\n",
        "print(f\"checkpoint saved to {unwrapped_checkpoint}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9boITmrA9_Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title run gradio interface\n",
        "\n",
        "command = (\n",
        "    \"python stable-audio-tools/run_gradio.py\"\n",
        "    \" --model-config /content/model_config.json\"\n",
        "    f\" --ckpt-path {unwrapped_checkpoint}\"\n",
        "    \" --share\"\n",
        ")\n",
        "\n",
        "!{command}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "CmDfN8Nj-CzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}